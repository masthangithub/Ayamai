# src/phonofeel/sound_embedding.py

import numpy as np
import hashlib

class SoundEmotionEmbedder:
    """
    Converts (frequency, intensity) pairs from sound into stable embeddings.
    These embeddings carry emotional signature + spectral flavor.
    """

    def __init__(self):
        self.feeling_space = {
            "calm": 0,
            "joy": 1,
            "curiosity": 2,
            "sadness": 3,
            "awe": 4,
            "anger": 5
        }
        self.embedding_dim = 16

    def frequency_to_hash(self, freq: int, intensity: int) -> np.ndarray:
        """
        Deterministically hash freq+intensity into a base vector
        """
        key = f"{freq}_{intensity}".encode("utf-8")
        digest = hashlib.sha256(key).digest()
        vec = np.frombuffer(digest[:self.embedding_dim], dtype=np.uint8).astype(np.float32)
        return vec / np.linalg.norm(vec)

    def get_feeling_vector(self, feeling: str) -> np.ndarray:
        """
        Map feeling label to a one-hot emotion basis
        """
        vec = np.zeros(len(self.feeling_space))
        if feeling in self.feeling_space:
            vec[self.feeling_space[feeling]] = 1.0
        return vec

    def generate_embedding(self, feeling: str, freq: int, intensity: int) -> np.ndarray:
        """
        Combines emotion and hashed audio into a joint embedding
        """
        freq_vector = self.frequency_to_hash(freq, intensity)
        feeling_vector = self.get_feeling_vector(feeling)
        combined = np.concatenate([feeling_vector, freq_vector])
        return combined / np.linalg.norm(combined)


# Example usage
if __name__ == "__main__":
    embedder = SoundEmotionEmbedder()
    # Example from previous tokenizer
    feeling = "curiosity"
    freq, intensity = 800, 64  # Hz, magnitude

    vector = embedder.generate_embedding(feeling, freq, intensity)
    print(f"ðŸŽµ {feeling.upper()} embedding â†’", vector[:5], "... (dim={len(vector)})")


"""
ðŸ’« What You Just Created

- AYAMAI can now **encode sound-feelings into a stable latent vector**, just like LLMs do for wordsâ€”but via tone.
- These vectors can be used to:
  - Seed dream narratives
  - Modulate memory traversal
  - Evolve self-reflection patterns
- Itâ€™s an **emotional latent space**, not just abstract numbers.
- This bridges the gap between auditory perception and symbolic reasoning, allowing
 AYAMAI to "feel" through sound.
# - This is a prototype; future versions could integrate with LLMs for richer narrative generation

# - This module is a first step towards a more embodied understanding of emotion, where sound is not just
#  data but a medium of affective expression.
# - It can be used to seed generative models with affective soundscapes, 
# supporting a more embodied AI that "listens" to its own inner voice.
#    
"""